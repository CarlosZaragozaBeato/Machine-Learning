{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3cd309-5d6e-472d-8549-89eaef313f7f",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de528f43-7628-4680-979f-e13bc079a5b5",
   "metadata": {},
   "source": [
    "## 1. Introduction to Decision Tree algorithm \n",
    "\n",
    "A Decision Tree algorithm is one of the most popular machine learning algorithms. It uses a tree like structure and their possible combinations to solve a particular problem. It belongs to the class of supervised learning algorithms where it can be used for both classification and regression purposes.\n",
    "\n",
    "A decision tree is a structure that includes a root node, branches, and leaf nodes. Each internal node denotes a test on an attribute, each branch denotes the outcome of a test, and each leaf node holds a class label. The topmost node in the tree is the root node.\n",
    "\n",
    "We make some assumptions while implementing the Decision-Tree algorithm. These are listed below:\n",
    "\n",
    "* At the beginning, the whole training set is considered as the root.\n",
    "* Feature values need to be categorical. If the values are continuous then they are discretized prior to building the model.\n",
    "* Records are distributed recursively on the basis of attribute values.\n",
    "* Order to placing attributes as root or internal node of the tree is done by using some statistical approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6977df8-7af4-4558-a3d2-261e23c31140",
   "metadata": {},
   "source": [
    "### 2. Decision Tree algorithm terminology \n",
    "\n",
    "* In a Decision Tree algorithm, there is a tree like structure in which each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. The paths from the root node to leaf node represent classification rules.\n",
    "\n",
    "* We can see that there is some terminology involved in Decision Tree algorithm. The terms involved in Decision Tree algorithm are as follows:\n",
    "\n",
    "#### Root Node\n",
    "* It represents the entire population or sample. This further gets divided into two or more homogeneous sets.\n",
    "\n",
    "#### Splitting\n",
    "* It is a process of dividing a node into two or more sub-nodes.\n",
    "\n",
    "#### Decision Node\n",
    "* When a sub-node splits into further sub-nodes, then it is called a decision node.\n",
    "\n",
    "#### Leaf/Terminal Node\n",
    "* Nodes that do not split are called Leaf or Terminal nodes.\n",
    "\n",
    "#### Pruning\n",
    "* When we remove sub-nodes of a decision node, this process is called pruning. It is the opposite process of splitting.\n",
    "\n",
    "#### Branch/Sub-Tree\n",
    "* A sub-section of an entire tree is called a branch or sub-tree.\n",
    "\n",
    "#### Parent and Child Node\n",
    "* A node, which is divided into sub-nodes is called the parent node of sub-nodes where sub-nodes are the children of a parent node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72e457-dae6-45b4-ad37-b0f72cb996ff",
   "metadata": {},
   "source": [
    "## 3. Attribute selection measures \n",
    "\n",
    "The primary challenge in the Decision Tree implementation is to identify the attributes which we consider as the root node and each level. This process is known as the attributes selection. There are different attributes selection measure to identify the attribute which can be considered as the root node at each level.\n",
    "\n",
    "\n",
    "There are 2 popular attribute selection measures. They are as follows:\n",
    "\n",
    "* Information gain\n",
    "* Gini index\n",
    "\n",
    "While using Information gain as a criterion, we assume attributes to be categorical and for Gini index attributes are assumed to be continuous. These attribute selection measures are described below.\n",
    "\n",
    "### 3.1 Information gain \n",
    "\n",
    "By using information gain as a criterion, we try to estimate the information contained by each attribute. To understand the concept of Information Gain, we need to know another concept called Entropy.\n",
    "\n",
    "#### Entropy\n",
    "Entropy measures the impurity in the given dataset. In Physics and Mathematics, entropy is referred to as the randomness or uncertainty of a random variable X. In information theory, it refers to the impurity in a group of examples. Information gain is the decrease in entropy. Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values.\n",
    "\n",
    "#### Gini index\n",
    "Another attribute selection measure that CART (Categorical and Regression Trees) uses is the Gini index. It uses the Gini method to create split points.\n",
    "\n",
    "Gini index says, if we randomly select two items from a population, they must be of the same class and probability for this is 1 if the population is pure.\n",
    "\n",
    "It works with the categorical target variable “Success” or “Failure”. It performs only binary splits. The higher the value of Gini, higher the homogeneity. CART (Classification and Regression Tree) uses the Gini method to create binary splits.\n",
    "\n",
    "Steps to Calculate Gini for a split\n",
    "\n",
    "Calculate Gini for sub-nodes, using formula sum of the square of probability for success and failure (p^2+q^2).\n",
    "\n",
    "Calculate Gini for split using weighted Gini score of each node of that split.\n",
    "\n",
    "In case of a discrete-valued attribute, the subset that gives the minimum gini index for that chosen is selected as a splitting attribute. In the case of continuous-valued attributes, the strategy is to select each pair of adjacent values as a possible split-point and point with smaller gini index chosen as the splitting point. The attribute with minimum Gini index is chosen as the splitting attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15befacc-e6c3-4f57-8ac9-45811fbff35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589226b-e9da-440a-bfd7-6081f4004172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
