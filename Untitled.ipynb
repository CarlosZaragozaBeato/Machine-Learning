{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97a3936-0b14-44fa-9c18-660dca4de9f8",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a technique used in machine learning to reduce the number of featrues in a dataset. This is typically done to remove noise, reduce the complexity of the model, and improve the accuracy and interpretability of the model. There are several techniques for dimensionality reduction, including:\n",
    "\n",
    "1. Principal component analysis (PCA): PCA is a linear dimensionality reduction technique that seeks to indentify the directions of maximum variance in the data. PCA transforms the original feature space into a new feature space with fewer dimensions, where each new dimension is a linear combination of the original feature.\n",
    "\n",
    "2. Linear Discriminant Analysis (LDA): LDA is a technique that seeks to project the data into a new feature space with fewer dimensions, while maximizing the separation between different classes in the data. It is a supervised technique, requiring the class labels to be known in advance.\n",
    "\n",
    "3. t-distributed stochastic neighbor embedding (t-SNE): t-SNE is a non-linear dimensionality reduction technique that seeks to embed high-dimensional data into a low-dimensional space, while preserving the local structure of the data. It is particularly useful for visualizing high-dimensional data.\n",
    "\n",
    "4. Autoenconder: A type of Neural network  used to learn a low dimensional representation of the input data in an unsupervised way by training a neural network to reconstruct the input.\n",
    "\n",
    "5. Random Projections: It involves projecting the high dimensional data into a lower dimensional space using a randomly generated matrix.\n",
    "\n",
    "6. Isomap. Multi-dimensional Scaling(MDS), UMAP\n",
    "\n",
    "Each technique has its own strngths and weaknesses, and the choice of technique depends on the specific requirements of the task and the nature of the data. In general, PCA is a goo technique to start with when the goal is to remove noise and reduce the complexity of the model. LDA is useful when the goal is to discriminate between different classes in the data, and t-SNE is useful for visualizing high-dimensional data.\n",
    "\n",
    "In the scikit-learn library, dimensionality reduction is implemented as set of classes in the `sklearn.decomposition` and `sklearn.manifold` modules. These classes provide a convenient and consistent interface for performing dimensionality reduction using a variety of techniques, including PCA, LDA, and t-SNE.\n",
    "\n",
    "For example, to perform PCA on a dataset using scikit-learn, you can use the `PCA` class from the `sklearn.decomposition` module. Here's an example of how you might use the `PCA` class to perform PCA on a dataset and reduce it to 2 dimensions: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ec52e7-5ae6-4a8c-a45e-b1986e04460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006e4537-f127-4d11-b31c-293bf1b93c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PCA object with the number of dimensions to keep\n",
    "\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a374e-807d-4eae-813a-a1cf19a4b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the PCA model to the data\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c78749-92f1-4242-a096-dd0a1e1ddc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to the new feature space\n",
    "X_space = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea1c73-424b-48e8-8b54-23b294810b96",
   "metadata": {},
   "source": [
    "Similarly, to perform LDA on a dataset using scikit-learn, you can use the `LinearDiscriminantAnalysis` class from the `sklearn.discriminant_analysis` module. Here's an example of how you might use the `LinearDiscriminantAnalysis` class to perform LDA on a labeled dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476a8abb-a71f-478e-a92a-3661f109f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2d290b-78e5-4cb7-985e-6da4a25c48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LDA object with the number of dimensions to keep\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b97b924-114e-40c2-b993-b40deac3ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the LDA model to the data\n",
    "lda.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fcfbb-43b5-4549-a16f-4c3577858350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to the new feature space\n",
    "X_lda = lda.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313becab-87f7-4361-b646-9a38615cdb47",
   "metadata": {},
   "source": [
    "And t-SNE is implement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
